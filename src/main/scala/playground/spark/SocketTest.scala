package playground.spark

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{ Seconds, StreamingContext }
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.SparkConf
import org.apache.spark.ui._
import org.apache.spark.Logging
import org.apache.log4j.{ Level, Logger }
import akka.dispatch.Foreach

object SocketTest {
  def main(args: Array[String]) {
    //    if (args.length < 2) {
    //      System.err.println("Usage: NetworkWordCount <hostname> <port>")
    //      System.exit(1)
    //    }

    //    StreamingExamples.setStreamingLogLevels()
    Logger.getRootLogger.setLevel(Level.WARN)
    // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[*]")
    val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream("localhost", 12345, StorageLevel.MEMORY_AND_DISK_SER)
    var mapLine = lines.map(decode)
    for (maps <- mapLine) {
      for (entry <- maps.collect) {
        entry match {
          case arr: Some[Array[(String, String)]] => {
            for (e <- arr.get) {
            	print(e._1 + "=" + e._2)
            }
          }
          case None => println("none")
        }
      }
    }
    //    var word = lines.flatMap(_.split(" "))
    //    var wordPair = word.map((_, 1))
    ////    var wardC = wordPair.reduceByKey(_ + _)
    //    
    //    
    //    val windowedWordCounts = wordPair.reduceByKeyAndWindow((a:Int,b:Int) => (a + b), Seconds(10), Seconds(5))
    //    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    //    windowedWordCounts.foreachRDD(rdd => {
    //      //      println("hello")
    //      for (s <- rdd.collect)
    //        println(s)
    //    })

    //    wardC
    //    lines.
    //    var pairs = lines.map((_, 1))
    //    pairs.print()
    //    val words = lines.flatMap(_.split(" "))
    //    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    //    wordCounts.print()
    //    for (line <- lines) {s
    ssc.start()
    ssc.awaitTermination()
  }

  //  var SOH = 0x01.toChar
  val SOH = " "
  def decode(str: String) = {
    try {
      var fields = str.split(SOH)
      val pairArray = for (field <- fields) yield {
        val pair = field.split("=")
        (pair(0), pair(1))
      }
      Some(pairArray)
    } catch {
      case _ => None
    }

  }

}